{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18071890_DL_CW2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bouracha/deeplearning/blob/master/18071890_DL_CW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "igD1TlZQ8JNr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Coursework 2\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "**Student Name:**  <Anthony Bourached> (Student Number: <18071890> )\n",
        "\n",
        "**Start date:** *4th March 2019*\n",
        "\n",
        "**Due date:** *18th March 2019, 09:00 am*\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "\n",
        "## How to Submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_DL_cw2.ipynb** before the deadline above.\n",
        "\n",
        "Please produce a **pdf** with all the **results** (tables and plots) as well as the **answers** to the questions below. For this assignment, you don't need to include any of the code in the pdf, but answers to the questions should be self-contained and should not rely on a code reference.  Page limit: 15 pg. \n",
        "\n",
        "### IMPORTANT\n",
        "Please make sure your submission includes **all results/answers/plots/tables** required for grading. We should not have to re-run your code."
      ]
    },
    {
      "metadata": {
        "id": "Qo1fx0F47hSL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FSSerqd-hmx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1: Memory Mechanisms (30 pts)\n",
        "\n",
        "Before going deeper into your practical tasks, take some time to revise and make sure you understand recurrent models (Vanilla RNN) and the two of major types of recurrent memory cells you will be using in this assignment: Long-Short Term Memory Units (LSTM) first introduced by Hochreiter and Schmidhuber [1997] and the more recent Gated Recurrent Units (GRU) by Cho et al. [2014].\n",
        "\n",
        "---------------------------------------------------\n",
        "\n",
        "### Models\n",
        "For convenience and to standardize notation please use the following notations:\n",
        "\n",
        "#### Vanilla RNN Model Eq.\n",
        "\n",
        "A simple RNN model we will be considering here is given by:\n",
        "\n",
        "$$h_t = \\tanh(W_h [h_{t-1}, x_t] + b_h)$$\n",
        "$$o_t = \\sigma(W_o h_t + b_o)$$\n",
        "\n",
        "#### LSTM Model Eq.\n",
        "\n",
        "The LSTM models we will be considering are given by: \n",
        "* For the input, forget and outputs gates are: \n",
        "\n",
        "$$f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)$$\n",
        "$$i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)$$\n",
        "$$o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$$\n",
        "\n",
        "* The internal state $c_t$ and hidden state $h_t$ are:\n",
        "\n",
        "$$c_t = f_t * c_{t-1} +  i_t * \\text{tanh}(W_c[h_{t-1}, x_t] + b_c) $$\n",
        "$$h_t = o_t * \\text{tanh}(c_t)$$\n",
        "\n",
        "#### GRU Model Eq.\n",
        "The GRU models we will be considering are given by:\n",
        "\n",
        "* The gates modulation $r_t$, $z_t$:\n",
        "$$z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z)$$\n",
        "$$r_t = \\sigma(W_r [h_{t-1}, x_t] + b_r)$$\n",
        "\n",
        "* The output and hidden state are now being given by:\n",
        "\n",
        "$$\\tilde{h}_t = \\text{tanh}(W_h [(h_{t-1} * r_t), x_t] + b_h)$$\n",
        "$$h_t = (1 - z_t)*\\tilde{h}_t + z_t * h_{t-1}$$\n",
        "\n",
        "---------------------------------\n",
        "### Simple Memory Task Description\n",
        "Consider the following memory task: Given an input sequence of $n$ numbers, we would like a system that, after reading this sequence (one number at a time) will always return the first number in the sequence. That is given an input sequence: $(x_1, x_2, \\cdots x_n)$, $x_i \\in \\mathbb{R}$ the system has to return, at time $t=n$ after 'reading' the last input $x_n$, the first input $x_1$.\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "###Questions\n",
        "Now consider the following questions:\n",
        "\n",
        "1. Given the task above, consider the above recurrent models (RNNs/LSTMs/GRUs). Which of these arhitectures can (theoretically) perfom the task above? In answering this questions, please consider a simple one-layer model of RNNs/GRU/LSTM with a one-dimensional input $x_t$, a $32$-dim hidden and output layer, followed by a transformation to a one-dimensional final output which should predict $x_0$. \\\\\n",
        "Whenever the answer is positive, give the gates' activations and weigths that will produce the desired behaviour. Whenever the answer is no, prove that there exists no such parameters that an arbitrary input sequence can be transformed to produce the first symbol read.  [15 pts]\n",
        "\n",
        "2. How does the answer change when now the desired output, after reading the whole sequence, is the 3rd input $x_3$ rather than $x_1$? Assume here $n>3$. [5 pts]\n",
        "\n",
        "3. How does the answer change when you restrict the inputs to be integers between $0$ and $31$ and we consider representing these inputs as a one-hot vectors uniquely representing these numbers? That is: for $x_i=0$, the input of the network would be $\\tilde{x}=[1,0,0,\n",
        "\\cdots, 0]$, for $x_i=1$, the input of the network would be $\\tilde{x}=[0,1,0,0,\\cdots, 0]$ and now predicted output should be $\\tilde{x}$. [5 pts]\n",
        "\n",
        "4. Are GRUs a special case of LSTMs? If so, give the expression of the GRU gates in term of LSTM's gates ($o_t, i_t, f_t$). If not, give a counter-example. This is a general question, not specific to the task above, although you can draw intution from the above questions. Assume here the same input. [5 pts]\n",
        "\n",
        "**Word limit: ** 1500 words or less\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nRMrp1Bs80XE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Answers:** \n",
        "1. We have a 32 dimensional output layer which we can convert through a non-linear function to our one dimensional output. Thus we can reduce the problem to one in which we want to make the output layer the same at each timestep and determined completely only by x1 and unchanged by all other timestep inputs. Thus we shall consider, for each model whether we can satisfy the above condition. \n",
        "\n",
        "-RNN: Our output vector is given by: $o_t = \\sigma(W_o h_t + b_o)$ to make this determined only by $x_1$ we need $h_t$ determined only by $x_1$. So consider: $h_t = \\tanh(W_{hh} h_{t-1} + W_{hx} x_t + b_h)$. For $t=1$ we need this term to be wholly determined by the $W_{hx} x_t$ term however for $t>1$ we need it wholly determined by the $W_{hh} h_{t-1}$ term. Since the weights are shared for each timestep, this is impossible. Thus it's a no for the RNN.\n",
        "\n",
        "-LSTM: Our output vector is given by $o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)$ (the hidden state carried over each timestep is proportional to this: $h_t = o_t * \\text{tanh}(c_t)$) which gives a linear (time shared weight) dependence on both $h_t$ and $x_t$ so it is not possible to save the first input x and not allow the others to contribute. UNLESS we are allowed to set $h_0 = x_1$, in this case the elements of the weight matrix $W_o$ which relate to $h_{t-1}$ can be set to 1 while those that relate to $x_t$ could be set to 0. Thus it is not theoretically possible for LSTM unless we know a priori what $x_1$ is. \n",
        "\n",
        " -GRU: We have $O_t = g(h_t)$ for some function g. We need $h_t = (1 - z_t)*\\tilde{h}_t + z_t * h_{t-1}$ to be only a function of $x_1$, thus since $\\tilde{h_t}$ is a function of $x_t$ we need it to contribute for for t=1 (ie need $z_1=0$) and not contribute for all $t>1$ (ie $z_t=1$ for all $t>1$) hence $h_t=h_{t-1}$ for all $h_t$ $t>1$ and $h_1 = x_1$. This is achieved by setting $b_z$ to a very large number and setting $h_0 = - 2*b_z$. The elements of $W_z$ that multiply by $h_t$ should be 1. In this case we will have that $z_1 = 0$ and $z_t = 1$ for $t>1$ since $h_2$ will be equal to $x_1$ and no longer this large negative number. Hence enabling $z_t$ to be dominated by the large, positive values in $b_z$. The differnce between LSTM and GRU in this regard is that the hidden state is carried in the GRU case can be independent of the current input. Though this maks a difference in this hypothetical scenario, in practical terms they are very similar\n",
        " \n",
        " 2. For the case of the vanilla RNN and LSTM the situation is trivially unchanged. If it is impossible to remember $x_1$ then it is impossible to remember $x_3$ where we only have a further disadvantage that we can't choose the incoming hidden representation in a favourable way. As for GRU; it was possible in the previous case because having control over the intial hidden state enabled us to, for the first iteration, effectively set $z_t$ to a value that it wouldn't otherwise take on with the large positive bias we required to ensure that $h_t=h_{t-1}$. However since we can't insist on the values of $h_2$ or $x_2$ taking on large negative values we won't be able to insist on the $z_t$ being 0 for $t=3$ and then 1 thereafter. Thus thisn is now not possible.\n",
        "\n",
        "3. RNNs and LSTMs are still not possible for the same reason. GRU is also unchanged.\n",
        "\n",
        "4. No because the fully gated GRU has different equations compared to LSTM (according to quora), so the case in coursera is a specific case?\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "oKtf7IbKAGx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cv-gzyMF8HH7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title IMPORTS\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JmoSOYwT40gT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q2: Training Memory Models (33 pts)\n",
        "Let us now investigate in pratice the above problem. And see how each these models does in learning the simple memory task proposed in Q1. For the purposed of this exercise we consider as inputs sequences of length $n=10$, with every input $x_i \\sim \\mathcal{N(0,1)}$ drawn (independently) from a unit Gaussian.\n",
        "\n",
        "\n",
        "1. **[5 pts]** Consider one of the models in **Q1.1** that you now know can represent a solution to our simple memory task. For this model, you know at least one set of weights what should implement exactly the prediction of the first read symbol $x_1$. Implement this model in tensorflow, with the fixed set of weights you previously inferred and show emperically that this indeed solves our task. \\\\\n",
        "  i) Report the (empirical) mean-squared error over $300$ randomly generated sequences. \\\\\n",
        "  ii) Is this set of weights a fix point for the learning? \n",
        "\n",
        "**Answers:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "cxLntRA38YFf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRU(object):\n",
        "    def __init__(self):\n",
        "        self.Wz = tf.get_variable('Wz',dtype=tf.float32,shape=[HIDDEN_UNITS + 1, HIDDEN_UNITS], initializer=tf.contrib.layers.xavier_initializer())\n",
        "        self.bz = tf.get_variable('bz',dtype=tf.float32,shape=[1,HIDDEN_UNITS], initializer=tf.zeros_initializer())     \n",
        "\n",
        "        self.Wr = tf.get_variable('Wr',dtype=tf.float32,shape=[HIDDEN_UNITS + 1, HIDDEN_UNITS], initializer=tf.contrib.layers.xavier_initializer())\n",
        "        self.br = tf.get_variable('br',dtype=tf.float32,shape=[1, HIDDEN_UNITS], initializer=tf.zeros_initializer())\n",
        "               \n",
        "        self.h = tf.get_variable('h',dtype=tf.float32,shape=[1, HIDDEN_UNITS], initializer=tf.zeros_initializer()) \n",
        "        \n",
        "        self.Wh = tf.get_variable('Wh',dtype=tf.float32,shape=[HIDDEN_UNITS + 1, HIDDEN_UNITS], initializer=tf.contrib.layers.xavier_initializer())\n",
        "        self.bh = tf.get_variable('bh',dtype=tf.float32,shape=[1, HIDDEN_UNITS], initializer=tf.zeros_initializer())        \n",
        "    \n",
        "    def step(self, xt):\n",
        "        self.z = tf.math.sigmoid(tf.matmul(tf.concat([self.h, xt], axis=1), self.Wz) + self.bz)\n",
        "        self.r = tf.math.sigmoid(tf.matmul(tf.concat([self.h, xt], axis=1), self.Wr) + self.br)\n",
        "      \n",
        "        self.xt = xt\n",
        "        self.h_hat = tf.math.tanh(tf.matmul(tf.concat([tf.math.multiply(self.h, self.r), xt], axis=1), self.Wh) + self.bh)\n",
        "        \n",
        "        self.h = tf.math.multiply((1 - self.z), self.h_hat) + tf.math.multiply(self.z, self.h)\n",
        "        return self.h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wKO8NXJyFeOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "135c5212-5328-46d4-dc3d-3af07b9703ec"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "SEQ_LENGTH = 10\n",
        "HIDDEN_UNITS = 32\n",
        "\n",
        "dummy_input = [tf.constant([[np.random.normal()]]) for _ in range(SEQ_LENGTH)]\n",
        "\n",
        "model = GRU()\n",
        "\n",
        "#Set constant Weights as specified in Q1\n",
        "model.bz = tf.Variable(tf.ones([1, HIDDEN_UNITS]))*1e8\n",
        "model.h = -1e8*model.bz\n",
        "model.Wz = tf.Variable(tf.ones([HIDDEN_UNITS + 1, HIDDEN_UNITS]))\n",
        "model.br = -tf.Variable(tf.ones([1, HIDDEN_UNITS]))*1e16\n",
        "model.Wr = tf.Variable(tf.zeros([HIDDEN_UNITS + 1, HIDDEN_UNITS]))\n",
        "model.bh = tf.Variable(tf.zeros([1, HIDDEN_UNITS]))\n",
        "model.Wh = tf.Variable(tf.ones([HIDDEN_UNITS + 1, HIDDEN_UNITS]))\n",
        "\n",
        "for i in range(0, SEQ_LENGTH):\n",
        "  h = model.step(dummy_input[i])\n",
        "y_T = tf.atanh(h)\n",
        "\n",
        "init = tf.global_variables_initializer()  \n",
        "with tf.train.SingularMonitoredSession() as sess:\n",
        "  sess.run(init)\n",
        "  sess.run(y_T)\n",
        "  \n",
        "  x1 = dummy_input[0].eval(session=sess)\n",
        "  y = y_T.eval(session=sess)\n",
        "  \n",
        "  print(\"Original input: \", x1[0, 0])\n",
        "  print(\"Output at time T: \", y[0, 0])\n",
        "  \n",
        "  "
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "Original input:  1.8849522\n",
            "Output at time T:  1.8849512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8Ztz3YbGFQ4t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "PP9Y52dW8cXY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. **[10 pts] ** Consider the following models, with standard initializations (tensorflow defaults), except a few bias terms as specified below:\n",
        "  * MM1: RNN bias term $b_h = 0$\n",
        "  * MM2: LSTM with $b_f = 1$ \n",
        "  * MM3: LSTM with $b_f = 5$\n",
        "  * MM4: LSTM with $b_f = -2$\n",
        "  * MM5: GRU with  $b_z = 5$\n",
        "\n",
        "  Implement each of these models in tensorflow. Given the initialization conditions above, compute and plot the following gradients as a function of time: the gradient of the last hidden unit $h_T$, where $T=n$ which respect to the previous hidden units $h_t$ for $t < T$.\n",
        "\n",
        "  $$ \\left\\Vert \\frac{\\partial h_T}{\\partial h_t} \\right\\Vert_1 = \\sum_i \\left| \\frac{\\partial h_T[i]}{\\partial h_t} \\right|$$  \n",
        "  and the gradient of the loss which respect to the hidden units over time $h_t$, for $t<T$:\n",
        "    $$ \\left\\Vert \\frac{\\partial Loss}{\\partial h_t} \\right\\Vert_1$$  \n",
        "where the $Loss$ we are considering here is the mean-squares error: $Loss(\\textbf{x}) = (x_1-y_T)^2$ where $y_T$ is the output of the reccurent model at time $T=n$ after reading the last input $x_n$.\n",
        "\n",
        "  **[3 pts] ** Now consider an additional model:\n",
        "  * MM0: LSTM with $b_f = 1000$ and $b_c = -50$. \n",
        "\n",
        "  Report and plot the same gradients as above. What do you observe? What does this model correspond to? (Hint: How does the information flow when processing a sequence). What will be its behaviour in training?\n",
        "\n",
        "  **[5 pts] ** Increase the length of the input sequence to $n=25$. How do the above gradients change? (you can see by an argument or by running the experiments and explain what you see). How do you expect the training to progress in all of these models MM0-5?  \n",
        "\n",
        "**Answers:**"
      ]
    },
    {
      "metadata": {
        "id": "V4YxvISHkxVA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "SEQ_LENGTH = 10 #@param\n",
        "HIDDEN_UNITS = 32 #@param\n",
        "index_input_to_remember = 0 #@param\n",
        "derivative = 'loss' #@param\n",
        "\n",
        "# dummy input sequence\n",
        "dummy_input = [tf.constant([[np.random.normal()]]) for _ in range(SEQ_LENGTH)] \n",
        "\n",
        "rnn_types = {\n",
        "    'MM0': # constructor for MM0\n",
        "    'MM1': # constructor for MM1\n",
        "    'MM2': # constructor for MM2\n",
        "    'MM3': # constructor for MM3\n",
        "    'MM4': # constructor for MM4\n",
        "    'MM5': # constructor for MM5\n",
        "}\n",
        "\n",
        "depths = {rnn_type: [] for rnn_type in rnn_types} # [9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
        "grad_norms = {rnn_type: [] for rnn_type in rnn_types}\n",
        "\n",
        "for rnn_type in rnn_types:\n",
        "\n",
        "  # --------------------------------------\n",
        "  # Construct model rnn_types[rnn_type]\n",
        "\n",
        "\n",
        "  # --------------------------------------\n",
        "  # Pass input and unroll in time. \n",
        "  # i) Record the intermediate hidden states $h_i$ in rnn_hidden_at_time\n",
        "  rnn_hidden_at_time = []\n",
        "  # ii) Record the last hidden unit and the last output on which we are going to \n",
        "  # make the prediction\n",
        "  last_hidden = ...\n",
        "  hidden_last_output = ...\n",
        "  \n",
        "  # Transformation of the RNN output into prediction\n",
        "  w = tf.Variable(tf.contrib.layers.xavier_initializer()([HIDDEN_UNITS, 1]))\n",
        "  b = tf.Variable(tf.contrib.layers.xavier_initializer()([1]))\n",
        "  model_output = tf.matmul(hidden_last_output, w) + b\n",
        "\n",
        "  loss = tf.reduce_mean(\n",
        "    tf.square(model_output-dummy_input[remembered_input_index]))\n",
        "\n",
        "  gradients_at_time = []\n",
        "  for i in range(1, SEQ_LENGTH):\n",
        "    if derivarive == 'loss':\n",
        "      current_loss_gradient = tf.gradients(\n",
        "        loss, \n",
        "        rnn_hidden_at_time[i],   \n",
        "      )\n",
        "    else:\n",
        "      current_gradient = tf.gradients(\n",
        "        last_hidden, \n",
        "        rnn_hidden_at_time[i],   \n",
        "      )\n",
        "    gradients_at_time.append(current_gradient)\n",
        "\n",
        "  \n",
        "  init = tf.global_variables_initializer()  \n",
        "  with tf.train.SingularMonitoredSession() as sess:\n",
        "    sess.run(init)\n",
        "    gradients = sess.run(gradients_at_time)\n",
        "\n",
        "  for gid, grad in enumerate(gradients):\n",
        "    depths[rnn_type].append(len(gradients)-gid)    \n",
        "    grad_norms[rnn_type].append(np.linalg.norm(grad))\n",
        "\n",
        "plt.figure()\n",
        "for rnn_type in depths:\n",
        "  plt.plot(depths[rnn_type], grad_norms[rnn_type],\n",
        "           label=\"%s\" % rnn_type, alpha=0.7)\n",
        "plt.legend()  \n",
        "plt.ylabel(\"Gradient mangnitude\", fontsize=15)\n",
        "plt.xlabel(\"Steps through time ($t$)\", fontsize=15)\n",
        "plt.xlim((1, SEQ_LENGTH-1))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9Anll6KoSX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " 3. **[10 pts]** Based on the intuitions in the last questions, pick two models (one GRU model, one LSTM model) to train the simple memory task of remembering the first input of a sequence.\n",
        "   * Explain your choice of models and initializations.\n",
        "   * Train your models on this task and plot your train and testing loss (mean-sq. error).\n",
        "   * Report (test) accuracy at end of training.\n",
        "    * Comment on the results you obtained. What happens if we were to increase $n$? What we happen if we trained longer? What if we were trying to predict the $3rd$ symbol read?\n",
        "\n",
        "  **Inputs**: $\\textbf{x} = (x_1, x_2, ..., x_n)$ where $x_i \\sim \\mathcal{N(0,1)}$ (independently drawn) \\\\\n",
        "\n",
        "  **Output (expected)**: $x_1$\n",
        "\n",
        "  **Training**: \n",
        "    * Data: Generate $32*200$ random sequences of length $n=5$ for training and $256$ ones for testing.  \n",
        "    * Recommended hyperparameters: learning rate $lr=1e-3$, minibatch size = $32$, optimizer = Adam. (Feel free to manipulate these to get the best performance).\n",
        " \n",
        "\n",
        "**Answers:**"
      ]
    },
    {
      "metadata": {
        "id": "Hu9l_cDLsA3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uBIQY7dW0jMU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plt.figure()\n",
        "# for rnn_type in train_loss:\n",
        "#   plt.plot(train_loss[rnn_type],\n",
        "#            label=\"%s\" % rnn_type, alpha=0.7)\n",
        "#   plt.plot(test_loss[rnn_type],\n",
        "#            label=\"%s (test)\" % rnn_type, alpha=0.3)\n",
        "# plt.legend()  \n",
        "# plt.ylabel(\"$ Loss$\", fontsize=15)\n",
        "# plt.xlabel(\"Training iterations\", fontsize=15)\n",
        "# plt.xlim((1, NUM_ITERS-1))\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFTb__Wu5LIj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q3: Meta-learning via LSTM-s (37 pts)\n",
        "(Credits to Pedro Ortega for insipring the task and insights behind it)"
      ]
    },
    {
      "metadata": {
        "id": "ocAnQ9C85TzQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Task Description\n",
        "\n",
        "Consider the following generative model: \n",
        "* We have $4$ symbols that will be generates from a multinomial/categorical distribution, with parameters $\\textbf{p}=(p_1, p_2, p_3)$: symbol 1 is generated with probability $p_1$, symbol 2 is generated with probability $p_2$, symbol 3 with probability $p_3$.\n",
        "$$X \\sim Categorical(3, \\textbf{p})$$\n",
        "\n",
        "* The parameter vector $\\textbf{p}$ is drawn from a Dirichlet prior: $$\\textbf{p} \\sim Dirichlet(\\alpha)$$\n",
        "\n",
        "We are going to use the above to generate sequences (a continuous stream of data/observations), in the following way:\n",
        "* Step 1: We sample $\\textbf{p}$ from the prior\n",
        "* Step 2: Given this $\\textbf{p}$, for $T-1$ time-steps we will generate i.i.d observations by sampling one of the $4$ symbols from the categorical distribution induced by $\\textbf{p}$\n",
        "$$ (X_1, X_2, \\cdots X_{T-1}) , s.t. X_i \\sim Categorical(3, \\textbf{p})$$\n",
        "* Step 3: At the end of the sequence we append a fourth symbol (RESET): $(X_1, X_2, \\cdots X_T, RESET)$\n",
        "* Step 4: Return to Step 1 and resample $\\textbf{p}$. \n",
        "* Repeat this 'forever'.\n",
        "\n",
        "This will give rise to a continuous stream of data, of the form: $ x_1, x_2, \\cdots x_{T-1}, RESET, x_{T+1}, x_{T+2}, \\cdots x_{2T}, RESET, \\cdots, x_{kT+1}, x_{kT+2}, \\cdots x_{(k+1)T}, RESET, \\cdots$.\n",
        "\n",
        "Note: Data generation is provided for you in the cell below. (You just need to call the minibatch function to get a sequence of this form).\n",
        "\n",
        "### Model\n",
        "\n",
        "We are going to consider an simple LSTM (32 units hidden state) and present this (generated) sequence of data as an input. At each time step $t$ the LSTM will receive one bit of information (gets to observe the symbol corresponding to this time step) and needs to output the probability distribution for the next symbol. Thus, at time $t$ the LSTM get as input the symbol $x_t$ and will return a probability over the next state $P(x_{t+1}| x_{t}, LSTM_{t-1})$.\n"
      ]
    },
    {
      "metadata": {
        "id": "6ySQa8Hz2oQa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1) **Without running any experiments (5 pts)**, try to think about the following scenarios and answer these questions: \n",
        "* Consider we generate the data with $Dirichlet(\\alpha)$, where $\\alpha = (10.,10.,1.,)$. What do you think the LSTM model will learn, if anything? Remember we are effectively changing the prediction task, every time we are resampling the probability vector $\\textbf{p}$. * Hint: Think about the distribution over $\\textbf{p}$ that this prior induces. *  \n",
        "\n",
        "* What if we consider a more uniformative prior, like $\\alpha=(1.1, 1.1, 1.1)$?  \n",
        "\n",
        "\n",
        "\n",
        "* How does this (learning ability and generalization) depend on the length of the tasks $T$ and the unrolling length on the LSTM? It might be helpful to consider the two extremes: \n",
        "i) $T=1$ (we reset the task at every time step). What should the model learn in this case?, ii) $T=\\infty$ (we sample the task once and keep it forever). What should the model learn in this case? (Answer this for both previous priors)\n",
        "\n",
        "\n",
        "* Does this increase or decrease the complexity of the prediction problem? What about the ability to generalize to unseen $\\textbf{p}$ ?\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ICqAsPCJ7EX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2) **Time to check your intuitions (8 pts)** \n",
        "\n",
        "Implement a simple LSTM model of $32$ hidden units. This will take as input a one-hot description of the obsevation ($[1,0,0,0]$ for symbol 1. $[0,1,0,0]$ for symbol 2, $[0,0,1,0]$ for symbol 3, $[0,0,0,1]$ for the RESET symbol). This input is fed into the 32-unit LSTM and the LSTM output is processed as follows: $\\textrm{Relu} \\Rightarrow \\textrm{Fully connected Linear Layer }\\Rightarrow \\textrm{Relu} \\Rightarrow \\textrm{Fully connected Linear Layer} \\Rightarrow \\textrm{Output}$ . The model will be trained by cross-entropy on predicting the next symbol -- thus as opposed to the previous task, we have an output of the model at each time step that is trying to predict the next symbol.\n",
        "\n",
        "Train the following models:\n",
        "* T = 5, and T=30 with the data generated from a Dirichlet with $\\alpha = (1.,10.,1.)$. Unrolling length for the LSTM = 150. Minibatch size = 64. (M1, M2)\n",
        "* T = 5, and T=30 with the data generated from a Dirichlet with $\\alpha = (1.3, 1.3, 1.3)$ Unrolling length for the LSTM = 150. Minibatch size = 64. (M3, M4)\n",
        "\n",
        "Train the models for $1000$ iterations ($1000$ minibatches). Record the training and testing preformance (every 10-20 iterations). Plot the curves over training time. What do you observe? (Is this a curve smooth? Do any of them plateau?). **[2x4 pts]**\n"
      ]
    },
    {
      "metadata": {
        "id": "S-L_VS685Jpz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "|#@title Generate data function\n",
        "\n",
        "n_symbols = 4\n",
        "def get_data_per_task(number_samples_per_task=10, p=None, alpha=None):\n",
        "  if p == None:\n",
        "    # sample task\n",
        "    if alpha == None:\n",
        "      p = np.random.dirichlet((1.1, 1.1, 1.1), 1)[0]\n",
        "    else:\n",
        "      p = np.random.dirichlet(alpha, 1)[0]\n",
        "    \n",
        "  p = np.append(p, [0])\n",
        "  sample = np.random.multinomial(1, p, size = number_samples_per_task)\n",
        "  \n",
        "  sample = np.append(sample,[[0,0,0,1]], axis=0)\n",
        "  return sample\n",
        "  \n",
        "\n",
        "def get_data(ntasks, nsamples_per_task, p=None, alpha=None):\n",
        "  sample = []\n",
        "  for task_id in range(ntasks):\n",
        "    sample.append(get_data_per_task(number_samples_per_task = nsamples_per_task, p=p, alpha=alpha))\n",
        "  return np.concatenate(sample)\n",
        "\n",
        "\n",
        "def get_minibatch(batch_size, ntasks, nsamples_per_task, p=None, alpha=None):\n",
        "  sample = get_data(batch_size*ntasks, nsamples_per_task, p=p, alpha=alpha)\n",
        "  return np.reshape(sample, [batch_size, ntasks*(nsamples_per_task+1), n_symbols+1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mNHq3X-F2oQb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3) **Analysis results (10 pts)**\n",
        "\n",
        "In this section, we will investigate what the models have actaully learnt. For this we will generate a few test sequences:\n",
        "* *Test sequence 1*: generate a test sequence that changes tasks every T=5 steps from a a Dirichlet with $\\alpha = (1.,10.,1.,)$.\n",
        "* *Test sequence 2*: generate a test sequence that changes tasks every T=5 steps from a a Dirichlet with $\\alpha = (1.3,1.3,1.3,)$.\n",
        "* *Test sequence 3*: generate a test sequence that 'changes tasks' every T=5 steps, but keep sampling according to the same probability vector $\\textbf{p}=(0,0,1)$ (You can use any of the extreme here).\n",
        "* *Test sequence 4*: generate a test sequence that changes tasks every T=5 steps from a a Dirichlet with $\\alpha = (0.2,0.2,0.2)$.\n",
        "\n",
        "i) Test the preformance of M1 and M3 and these test sequences. In addition plot the actual prediction the models do (probability of symbols over time). This should give you more insight in what the model does. Does this correspond or contradict your previous intuitions? **[5 pts]**\n",
        "\n",
        "ii) Repeat the same procedure for task length $T=30$ and models M2 and M4. What do you observe? How do M2 and M4 compare to each other and how to their compare to M1 and M3 (the models trained on the shorter task length). **[5 pts]**\n"
      ]
    },
    {
      "metadata": {
        "id": "AaZ86X0O2oQb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4) **Comparison to the Bayesian update (14 pts)**\n",
        "\n",
        "\n",
        "Going back to the generative process in the task description. For a given prior, for each the mini-tasks (selecting/sampling a $\\textbf{p}$), one could compute the Bayesian posterior at each time step.  We start with a prior and every time we observe a symbol with update our posterior over the parameters\n",
        " $\\textbf{p}$ given the data. We do this every time step, till we reach the RESET symbol which marks the end of the task. Then we start again, from the prior.\n",
        ",\n",
        "\n",
        "i) Derive the posterior update for each time step. (Hint: since the two distribution are conjugates or each other, the posterior has a closed form). **[2 pts]**\n",
        "\n",
        "ii) Implement this posterior update and use it to infer the probabilities over the next symbol, for the previously generated test sequences. This will tell you, what the inferred probabilities would be, if we knew the structure of the model, the prior and that the reset symbol means the tasks has finished and we should reset our estimate to the prior. (For test sequence 1 and 2, use the prior that generated them, for test sequence 3 and 4 compute the updates starting from both priors) **[5 pts]**\n",
        "\n",
        "iii) Compare this to what the LSTM predictions are. What do you observe? What are the failure cases -- can you explain why this might happen? (For test sequence 1 and 2, use the prior that generated them, for test sequence 3 and 4 compute the updates starting from both priors). **[7 pts]**\n",
        " \n",
        "For this analysis, only consider $T=30$ and respectively models M2 and M4.\n"
      ]
    },
    {
      "metadata": {
        "id": "M9x2k6nqSNm1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}