{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18071890_DL_cw3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "5cfQlEC07pal",
        "BWded8OecPFg",
        "SQX64l4--uxU",
        "BF22sRaeBkla",
        "0V1KL4wK2Utm",
        "nIYXEQzeEhHk",
        "BhWHvrlBKkuJ",
        "_T1p-btINE9o",
        "S1y5KidNKNjo",
        "L_DF7_BZzDkW",
        "qFobF5vLznpb",
        "jEZwzU-hND3B"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bouracha/deeplearning/blob/master/18071890_DL_cw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "yDU798o0FDyp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning: Coursework 3\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "**Student Name:**  <Anthony Bourached> (Student Number: <18071890> )\n",
        "\n",
        "**Start date:** *26th March 2019*\n",
        "\n",
        "**Due date:** *29th April 2019, 09:00 am *\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "\n",
        "## How to Submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **studentnumber_DL_cw3.ipynb** before the deadline above.\n",
        "\n",
        "Please produce a **pdf** with all the **results** (tables and plots) as well as the **answers** to the questions below. For this assignment, you don't need to include any of the code in the pdf, but answers to the questions should be self-contained and should not rely on a code reference.  **Page limit: 20 pg.**\n",
        "\n",
        "### IMPORTANT\n",
        "Please make sure your submission includes **all results/answers/plots/tables** required for grading. We should not have to re-run your code.\n",
        "\n",
        "### Credits\n",
        "A special thank you to Mihaela Rosca, Shakir Mohammend and Andriy Mnih for their help in this coursework."
      ]
    },
    {
      "metadata": {
        "id": "jWcqFCOsFmzQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment Description \n",
        "(Latent Generative Models)\n"
      ]
    },
    {
      "metadata": {
        "id": "e8PEhbMy8Q-H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Topics and optimization techniques covered: \n",
        "\n",
        "* Stochastic variational inference\n",
        "* Amortized variational inference (VAEs)\n",
        "* Improving amortized variational inference using KL annealing\n",
        "* Improving amortized variational inference using constraint optimization\n",
        "* Avoiding latent space distribution matching using GANs\n"
      ]
    },
    {
      "metadata": {
        "id": "SPajjqSiGIrP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "Y2MQ912uAi4_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note: ** Before taking on this assignment you might find it useful to take a look at the `tensorflow_probability` [package](https://www.tensorflow.org/probability), especially if you have not used probability distributions in TensorFlow before. In this assignment we will use only standard probability distribtions (like Gaussian and Bernouli), but worth taking a look on how TF handles in-graph sampling and optimizations involving distributions."
      ]
    },
    {
      "metadata": {
        "id": "hk4fInRg8NWe",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Imports (Do not modify!)\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Plotting library.\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(rc={\"lines.linewidth\": 2.8}, font_scale=2)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Tensorflow probability utilities\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMOEsOiP9L4l",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "447b40e5-fa5f-4576-b3da-5d35230b9c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Check you're using the GPU (Expand me for instructions)\n",
        "# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ecSIgT73GAIg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper Functions for visualisation\n"
      ]
    },
    {
      "metadata": {
        "id": "-3loVhgk-H2b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gallery(array, ncols=10, rescale=False):\n",
        "    \"\"\"Data visualization code.\"\"\"\n",
        "    if rescale:\n",
        "      array = (array + 1.) / 2\n",
        "    nindex, height, width, intensity = array.shape\n",
        "    nrows = nindex//ncols\n",
        "    assert nindex == nrows*ncols\n",
        "    # want result.shape = (height*nrows, width*ncols, intensity)\n",
        "    result = (array.reshape(nrows, ncols, height, width, intensity)\n",
        "              .swapaxes(1,2)\n",
        "              .reshape(height*nrows, width*ncols, intensity))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jyRPYX3WWnPZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_digits(axis, digits, title=''):\n",
        "  axis.axis('off')\n",
        "  ncols = int(np.sqrt(digits.shape[0]))\n",
        "  axis.imshow(gallery(digits, ncols=ncols).squeeze(axis=2), \n",
        "                 cmap='gray')\n",
        "  axis.set_title(title, fontsize=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nsKLwGBa5iWz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_latent_interpolations(generator, prior, session):\n",
        "  a = np.linspace(0.0, 1.0, BATCH_SIZE)\n",
        "  a = np.expand_dims(a, axis=1)\n",
        "\n",
        "  first_latents = prior.sample()[0]\n",
        "  second_latents = prior.sample()[0]\n",
        "\n",
        "  # To ensure that the interpolation is still likely under the Gaussian prior,\n",
        "  # we use Gaussian interpolation - rather than linear interpolation.\n",
        "  interpolations =  np.sqrt(a) * first_latents + np.sqrt(1 - a) * second_latents\n",
        "\n",
        "  ncols = int(np.sqrt(BATCH_SIZE))\n",
        "  samples_from_interpolations = generator(interpolations)\n",
        "  samples_from_interpolations_np = sess.run(samples_from_interpolations)\n",
        "  plt.gray()\n",
        "  axis = plt.gca()\n",
        "  show_digits(\n",
        "     axis, samples_from_interpolations_np, title='Latent space interpolations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQOPrU_tfljy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters (Do not modify!)\n",
        "These were chosen to work across all models you are going to be training. At times you will need to explore other configuration to answer the questions, but keep this as default -- things should train nicely under these parameters! Check your model and gradients if that is not the case! "
      ]
    },
    {
      "metadata": {
        "id": "iFjsBLsDCCAC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_LATENTS = 10\n",
        "TRAINING_STEPS = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W528LifA2-mW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5cfQlEC07pal",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Data\n",
        "\n",
        "### Handwritten Digit Recognition Dataset (MNIST)\n",
        "\n",
        "\n",
        "We will be revisiting the [MNIST digit dataset](https://yann.lecun.com/exdb/mnist/) for this assignment. The setup/processing of the data will be a bit different in this assignment as for training purposes it is sometimes easier to expose the data sampling as an operation in the graph, rather than going through placeholders. This is in general a very useful way of handling data in tensorflow, especially for larger training regimes where 'stepping out' of the graph might be very expensive.\n",
        "\n",
        "In the following we will walk you through how to get the data into this form. You do not need to worry about it, but it is worth making sure you understand the step, as this is something that might be useful to replicate in the future.\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "5EHWPAsd7u50",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWipXtlq8Ya9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(mnist.train.images.shape)\n",
        "print(type(mnist.train.images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BWded8OecPFg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transform the data from numpy arrays to in graph tensors.\n",
        "\n",
        "This allows us to use TensorFlow datasets, which ensure that a new batch from the data is being fed at each session.run. This means that we do not need to use feed_dicts to feed data to each session."
      ]
    },
    {
      "metadata": {
        "id": "QmrdDH2ibvGG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_tf_data_batch(np_data, shuffle=True):  \n",
        "  # Reshape the data to image size.\n",
        "  images = np_data.reshape((-1, 28, 28, 1))\n",
        "  \n",
        "  # Create the TF dataset.   \n",
        "  dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "  \n",
        "  # Shuffle and repeat the dataset for training.\n",
        "  # This is required because we want to do multiple passes through the entire\n",
        "  # dataset when training.\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(100000).repeat()\n",
        "    \n",
        "  # Batch the data and return the data batch.\n",
        "  one_shot_iterator = dataset.batch(BATCH_SIZE).make_one_shot_iterator()\n",
        "  data_batch = one_shot_iterator.get_next()\n",
        "  return data_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kKwY_jfDHZdy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)\n",
        "print(real_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3gJS2FpELaZs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1: Latent Variable models and Variational Inference\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eqamNxQx9xxb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## T1.1 Stochastic Variational Inference\n",
        "\n",
        "In this first task we will consider a simple latent variable model $z \\rightarrow x$. Your task is to use stochastic variational inference to train a generative model on the MNIST data. For each data point $x_i$, there is a set of variational parameters to be learned. Throughout this assessment, the posterior and the prior will be Normal random variables, with uncorrelated dimensions.\n",
        "\n",
        "\n",
        "Objective - maximize: \n",
        "\\begin{equation}\n",
        " \\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)}{\\left[ \\log p_\\theta(x|z)\\right]}  - \\mathbb{E}_{p^*(x)} \\left[KL(q(z|x)||p(z))\\right]\n",
        "\\end{equation}\n",
        "\n",
        "For more information, please check out:\n",
        " * http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf\n",
        "\n",
        "**Task**: Implement and train this model to generate MNIST digits. Visualise the results and answer the questions at the end of the section."
      ]
    },
    {
      "metadata": {
        "id": "TAJoDy9u6i43",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Implementation\n",
        "\n",
        "In the following, I am going to walk you through implementing this model. This will only be done for the first task, but you can use this to structure your code for all of the tasks after this. Also worth taking a look at question 1 at the end of this section before finishing the implementation - this should give you exactly what you have to implement in the update operations and training loop."
      ]
    },
    {
      "metadata": {
        "id": "_gUUEfvW9cbq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data variable\n",
        "\n",
        "We will do multiple session.run to update the variational parameters for one data batch.\n",
        "To ensure that the same batch is used, we define a variable for the data, and update it after updating the decoder parameters."
      ]
    },
    {
      "metadata": {
        "id": "aqDaA7--9tLQ",
        "colab_type": "code",
        "outputId": "da70d07a-9bc0-4e31-dd07-edc5f67827de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "data_var = tf.Variable(\n",
        "      tf.ones(shape=(BATCH_SIZE, 28, 28, 1), dtype=tf.float32), \n",
        "      trainable=False)\n",
        "\n",
        "data_assign_op = tf.assign(data_var, real_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UqJLmAhoCy0M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the decoder"
      ]
    },
    {
      "metadata": {
        "id": "S62pxDHf2C_t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DECODER_VARIABLE_SCOPE = \"decoder\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nXI5bRIXqmdV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def standard_decoder(z):\n",
        "  with tf.variable_scope(DECODER_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
        "    h = tf.layers.dense(z, 7 * 7 * 64, activation=tf.nn.relu)\n",
        "    h = tf.reshape(h, shape=[BATCH_SIZE, 7, 7, 64])\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=32,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=1,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=None,  # Do not activate the last layer.\n",
        "          padding='same')(h)\n",
        "    return tf.distributions.Bernoulli(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UIJMW6huDFd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder = standard_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OgjQngbeKXG_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define prior"
      ]
    },
    {
      "metadata": {
        "id": "idi-DgDzl_iE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def multi_normal(loc, log_scale):\n",
        "  # We model the latent variables as independent\n",
        "  return tfd.Independent(\n",
        "      distribution=tfd.Normal(loc=loc, scale=tf.exp(log_scale)),\n",
        "      reinterpreted_batch_ndims=1)\n",
        "      \n",
        "def make_prior():\n",
        "  # Zero mean, unit variance prior.\n",
        "  prior_mean = tf.zeros(shape=(BATCH_SIZE, NUM_LATENTS), dtype=tf.float32)\n",
        "  prior_log_scale = tf.zeros(shape=(BATCH_SIZE, NUM_LATENTS), dtype=tf.float32)\n",
        "\n",
        "  return multi_normal(prior_mean, prior_log_scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aQL-Olb7u07w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prior = make_prior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-qSI63aKhTa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define variational posterior $q(z|x)$\n",
        "\n",
        "Define this to be a multi-dimensional Gaussian distribution. You can use the helper function above for this, but keep in mind the parameters of this distribution (mean and variance) ought to be trained."
      ]
    },
    {
      "metadata": {
        "id": "dnb7c0ldsw4h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Build the variational posterior\n",
        "\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# variational_posterior = multi_normal(....)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pIdtEU_OKpuM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define and build optimization objective (ELBO)\n",
        "\n",
        "Putting things together: build the likelihood term and the KL term in the objective in (T1.1) description. "
      ]
    },
    {
      "metadata": {
        "id": "JheUZIkxvlDl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bound_terms(data_batch, variational_posterior, decoder_fn):\n",
        "\n",
        "  ##################\n",
        "  # YOUR CODE HERE #\n",
        "  ##################\n",
        "  \n",
        "  # Reduce mean over the batch dimensions\n",
        "  likelihood_term = tf.reduce_mean(likelihood_term)\n",
        "\n",
        "  ##################\n",
        "  # YOUR CODE HERE #\n",
        "  ##################\n",
        "\n",
        "  # Reduce over the batch dimension.\n",
        "  kl_term = tf.reduce_mean(kl_term)\n",
        "  \n",
        "  # Return the terms in the optimization objective in (1.1) description\n",
        "  return likelihood_term, kl_term"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iG-UEr2xwZB7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Maximize the data likelihodd and minimize the KL divergence between the prior and posterior\n",
        "likelihood_term, kl_term = bound_terms(data_var, variational_posterior, decoder)\n",
        "train_elbo = likelihood_term - kl_term\n",
        "\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# loss =  ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vZ6pW0Kj5cTY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the update operations for the variational and global variables"
      ]
    },
    {
      "metadata": {
        "id": "reMfmOhuzhEo",
        "colab_type": "code",
        "outputId": "35df4a6d-da3d-479b-ff96-535ebefd7262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "# Variational variable optimizer\n",
        "variational_vars_optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
        "\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# variational_vars = ... # list of variational variables\n",
        "\n",
        "# Just to check\n",
        "print('Variational vars\" {}'.format(variational_vars))\n",
        "variational_vars_update_op = variational_vars_optimizer.minimize(\n",
        "      loss, var_list=variational_vars)\n",
        "\n",
        "\n",
        "# Decoder optimizer\n",
        "decoder_optimizer = tf.train.AdamOptimizer(0.001, beta1=0.9, beta2=0.9)\n",
        "decoder_vars = tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=DECODER_VARIABLE_SCOPE)\n",
        "print('Decoder vars\" {}'.format(decoder_vars))\n",
        "decoder_update_op = decoder_optimizer.minimize(loss, var_list=decoder_vars)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variational vars\" [<tf.Variable 'mean:0' shape=(64, 10) dtype=float32_ref>, <tf.Variable 'scale:0' shape=(64, 10) dtype=float32_ref>]\n",
            "Decoder vars\" [<tf.Variable 'decoder/dense/kernel:0' shape=(10, 3136) dtype=float32_ref>, <tf.Variable 'decoder/dense/bias:0' shape=(3136,) dtype=float32_ref>, <tf.Variable 'decoder/conv2d_transpose/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>, <tf.Variable 'decoder/conv2d_transpose/bias:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'decoder/conv2d_transpose_1/kernel:0' shape=(5, 5, 1, 32) dtype=float32_ref>, <tf.Variable 'decoder/conv2d_transpose_1/bias:0' shape=(1,) dtype=float32_ref>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mURSB8e_Duw-",
        "colab_type": "code",
        "outputId": "758086da-da20-4d19-87e1-9d4faf67e90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "cell_type": "code",
      "source": [
        "# Check trainable variables\n",
        "tf.trainable_variables()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'mean:0' shape=(64, 10) dtype=float32_ref>,\n",
              " <tf.Variable 'scale:0' shape=(64, 10) dtype=float32_ref>,\n",
              " <tf.Variable 'decoder/dense/kernel:0' shape=(10, 3136) dtype=float32_ref>,\n",
              " <tf.Variable 'decoder/dense/bias:0' shape=(3136,) dtype=float32_ref>,\n",
              " <tf.Variable 'decoder/conv2d_transpose/kernel:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
              " <tf.Variable 'decoder/conv2d_transpose/bias:0' shape=(32,) dtype=float32_ref>,\n",
              " <tf.Variable 'decoder/conv2d_transpose_1/kernel:0' shape=(5, 5, 1, 32) dtype=float32_ref>,\n",
              " <tf.Variable 'decoder/conv2d_transpose_1/bias:0' shape=(1,) dtype=float32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 303
        }
      ]
    },
    {
      "metadata": {
        "id": "ped8r03W4hR_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ]
    },
    {
      "metadata": {
        "id": "42gyWqEl892z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Number of SVI updates per sample\n",
        "NUM_SVI_UPDATES = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jG3W2CZaCSxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kHE6WOvO6SbG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %hide_pyerr  # - uncomment to interrupt training without a stacktrace\n",
        "losses = []\n",
        "kls = []\n",
        "likelihood_terms = []\n",
        "\n",
        "for i in xrange(TRAINING_STEPS):\n",
        "  \n",
        "  # Update the data batch.\n",
        "  sess.run(data_assign_op)\n",
        "\n",
        "  # Training (put things together based on the operations you've defined before)\n",
        "  ##################\n",
        "  # YOUR CODE HERE #\n",
        "  ##################\n",
        "  \n",
        "  # Report the loss and the kl once in a while.\n",
        "  if i % 10 == 0:\n",
        "    iteration_loss, iteration_kl, iteration_likelihood = sess.run(\n",
        "          [loss, kl_term, likelihood_term])\n",
        "    print('Iteration {}. Loss {}. KL {}'.format(\n",
        "          i, iteration_loss, iteration_kl))\n",
        "    losses.append(iteration_loss)\n",
        "    kls.append(iteration_kl)\n",
        "    likelihood_terms.append(iteration_likelihood)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-_OUSMQTK-4t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Let us take a look at the optimization process and the resulting model"
      ]
    },
    {
      "metadata": {
        "id": "vy37JkDSDEEN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize training process\n",
        "\n",
        "Plot the loss and KL over the training process (number of iterations)"
      ]
    },
    {
      "metadata": {
        "id": "qKhjrxHL-FHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(2*8,5))\n",
        "\n",
        "axes[0].plot(losses, label='Negative ELBO')\n",
        "axes[0].set_title('Time', fontsize=15)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(kls, label='KL')\n",
        "axes[1].set_title('Time', fontsize=15)\n",
        "axes[1].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WyPiTnMV9WTE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate samples, reconstructions and latent interpolation"
      ]
    },
    {
      "metadata": {
        "id": "e5sslysXwrAj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read data (just sample from the data set)\n",
        "# real_data_examples\n",
        "\n",
        "# Note: the reconstructions are only valid after the inner loop optimization has\n",
        "# been performed.\n",
        "# reconstructions = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pVbYV_G6DKhw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Sample from the generative model!\n",
        "# final_samples = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JKpqt2Q-FwqH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*4,4))\n",
        "\n",
        "show_digits(axes[0], real_data_examples, 'Data')\n",
        "show_digits(axes[1], data_reconstructions, 'Reconstructions')\n",
        "show_digits(axes[2], final_samples, 'Samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jCsMRvKP9NtX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_latent_interpolations(lambda x: decoder(x).mean(), prior, sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-FgmMbyIToW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1.1 VI Questions (28 pts):\n",
        "\n",
        "We going to go through some questions on the model you have just implemented. The first question here could be answered before the implementation and can act as a blue-print for how to do the training. We are going to spend a bit more time on this first method as it is paramount you understand the optimization process here as a lot of the other tasks build on top of this one. \n",
        "\n",
        "Whenever a question asks for an effect/behaviour when varying one of the conditions, feel free to experiment. Both theoretical arguments and emperical plots showing the relevant behaviour will be accepted here.\n",
        "\n",
        "1. **[5 pts]**  Derive the variational ELBO for **one** data point $x$ and explain how one would update the parameters for variational posterior $q_{\\phi}(z|x)$, as well as the parameters of the generative distribution $p_{\\theta}(x|z)$. Assume a Gaussian prior and a multi-dim Gaussian variational prior, as well as the generative function given  by the decoder in the code above. This is basically outlining the optimization you should be implementing in the **Training loop** section above. \n",
        "\n",
        "2.  **[6 pts] **In the **Build the update operations for the variational and global variables**, I have define two separate optimizers for the two sets of parameters ($\\theta$ and $\\phi$). \n",
        "  * i)  How would you implement this with just one optimizer? (You just need to\n",
        "explain how you would do it, but not implement).\n",
        "  * ii) What happen if we change the variational variables' optimizer `variational_vars_optimizer` to `tf.train.AdamOptimizer`? (Feel free to experiment and change the learning rate accordingly) \n",
        "\n",
        "3. **[2 pts]** What are the computational considerents to think of when using SVI? What would happen if you would now want to train this SVI model on a big dataset, such as ImageNet? What part of this optimization process is mostly affected and in which way?\n",
        "\n",
        "\n",
        "4. ** [2 pts]**What is the effect of the number of SVI updates on the ELBO and on the KL term?\n",
        "\n",
        "5. **[3 pts]** What is the effect of the data batch size on the convergence speed compared to the effect of the number of SVI updates? What is the effect of the number of training steps for the decoder compared to the number of SVI steps per decoder update?\n",
        "\n",
        "\n",
        "**[10 pts] Model Implementation and Results**"
      ]
    },
    {
      "metadata": {
        "id": "E54OTbDJ91h2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## T1.2 Amortized Variational Inference\n",
        "\n",
        "**Reminder**: The idea behind amortized inference is to replace the slow iterative optimization process we needed to do in the previous method for each data-point, with a faster non-iterative one.  Check the lecture slides and/or references below for more details.\n",
        "\n",
        "Thus, instead of learning one set of posterior variables per data point, we can use function approximation to learn the distributional variables. Specifically, the posterior parameters for $x_i$ will be the output of a *learned* function $f_\\theta(x_i)$, where $\\theta$ are parameters now shared across all data points. Can you think of why this is useful?\n",
        "\n",
        "\n",
        "<h2 align=\"center\"></h2> <img src=\"http://elarosca.net/vae.png?format=100w\" width=500 >\n",
        "\n",
        "\n",
        "Objective - maximize: \n",
        "\\begin{equation}\n",
        " \\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} {\\left[ \\log p_\\theta(x|z) \\right]}  - \\mathbb{E}_{p^*(x)} \\left[ KL(q(z|x)||p(z)) \\right]\n",
        "\\end{equation}\n",
        "\n",
        "For more information, please check out: \n",
        "  * https://arxiv.org/abs/1312.6114\n",
        "\n",
        "\n",
        "**Task**: Implement and train this model to generate MNIST digits. Visualise the results and answer the questions at the end of the section."
      ]
    },
    {
      "metadata": {
        "id": "SQX64l4--uxU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "metadata": {
        "id": "D6-Bxsb6mW5E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EbRi8ejYmezC",
        "colab_type": "code",
        "outputId": "f6785a03-8e44-4423-b5b0-ac59d7557cde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)\n",
        "print(real_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ZKeo8iqzeQ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the encoder"
      ]
    },
    {
      "metadata": {
        "id": "Dsn12nG3NQKg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ENCODER_VARIABLE_SCOPE = 'encoder'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ys5kCiS3NKQo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encoder(x):\n",
        "  with tf.variable_scope(ENCODER_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
        "    \n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=8,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(x)\n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=16,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=32,\n",
        "          kernel_size=5,\n",
        "          strides=1,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    \n",
        "    out_shape = 1\n",
        "    for s in h.shape.as_list()[1:]:\n",
        "      out_shape*= s\n",
        "    \n",
        "    h = tf.reshape(h, shape=[BATCH_SIZE, out_shape])\n",
        "    mean = tf.layers.dense(h, NUM_LATENTS, activation=None)\n",
        "    scale = tf.layers.dense(h, NUM_LATENTS, activation=None)\n",
        "    return multi_normal(loc=mean, log_scale=scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nq9iLKwizhke",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the prior"
      ]
    },
    {
      "metadata": {
        "id": "2mv1BbElmQnR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# prior = ... "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JjguoGaJzluC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the variational posterior\n",
        "**Note**: We no longer have to use a variable to store the data. We will perform one encoder update per decoder update, so it is OK for the data batch to be refreshed at each run."
      ]
    },
    {
      "metadata": {
        "id": "Uy-QiXwSOGXY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# variational_posterior = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fN-ABOl0z_t9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the decoder\n",
        "We will use the same decoder as in T1.1"
      ]
    },
    {
      "metadata": {
        "id": "6qm4Oc-10IGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder = standard_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C5jFI2fM0Dvf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define and build optimization objective (ELBO)"
      ]
    },
    {
      "metadata": {
        "id": "FaW-j91gOuU0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Maximize the data likelihodd and minimize the KL divergence between the prior \n",
        "# and posterior. We use the exact same loss as in the SVI case.\n",
        "\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# likelihood_term, kl_term = ...\n",
        "train_elbo = likelihood_term - kl_term\n",
        "# loss =  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhavmPWq0iNw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define optimization and the update operations"
      ]
    },
    {
      "metadata": {
        "id": "N2BquI1DPF3G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer(0.001, beta1=0.9, beta2=0.9)\n",
        "\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# update_op = optimizer.minimize(loss, var_list=...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2xlVu9cRPg6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "VdB-pHnQPcuU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrGGwzfDPdtb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %hide_pyerr  # - uncomment to interrupt training without a stacktrace\n",
        "losses = []\n",
        "kls = []\n",
        "likelihood_terms = []\n",
        "\n",
        "\n",
        "for i in xrange(TRAINING_STEPS):\n",
        "  \n",
        "\n",
        "  ##################\n",
        "  # YOUR CODE HERE #\n",
        "  ##################\n",
        "  # Training, use update_op \n",
        "  \n",
        "  \n",
        "  if i % 100 == 0:\n",
        "    iteration_loss, iteration_likelihood, iteration_kl  = sess.run(\n",
        "          [loss, likelihood_term, kl_term])\n",
        "    print('Iteration {}. Loss {}. KL {}'.format(\n",
        "          i, iteration_loss, iteration_kl))\n",
        "    losses.append(iteration_loss)\n",
        "    kls.append(iteration_kl)\n",
        "    likelihood_terms.append(iteration_likelihood)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5-kenjtA05Gp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Let us take a look at the optimization process and the resulting model"
      ]
    },
    {
      "metadata": {
        "id": "WO8-O_MVp9h3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Visualize the loss in time"
      ]
    },
    {
      "metadata": {
        "id": "XlA-gGsnqA-Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*8,5))\n",
        "\n",
        "axes[0].plot(losses, label='Negative ELBO')\n",
        "axes[0].set_title('Time', fontsize=15)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(kls, label='KL')\n",
        "axes[1].set_title('Time', fontsize=15)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(likelihood_terms, label='Likelihood Term')\n",
        "axes[2].set_title('Time', fontsize=15)\n",
        "axes[2].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zs7PaG_j9Gac",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate samples and latent interpolations"
      ]
    },
    {
      "metadata": {
        "id": "5KtRZWKaO6IY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples = decoder(prior.sample()).mean()\n",
        "samples.shape.assert_is_compatible_with([BATCH_SIZE, 28, 28, 1])\n",
        "\n",
        "reconstructions = decoder(variational_posterior.sample()).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jz0anr_cPskE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data_vals, final_samples_vals, data_reconstructions_vals = sess.run(\n",
        "      [real_data, samples, reconstructions])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJIu_K8KP5tE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*4,4))\n",
        "\n",
        "\n",
        "show_digits(axes[0], real_data_vals, 'Data')\n",
        "show_digits(axes[1], data_reconstructions_vals, 'Reconstructions')\n",
        "show_digits(axes[2], final_samples_vals, 'Samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yumeCpiF9Dj7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_latent_interpolations(lambda x: decoder(x).mean(), prior, sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kUuLShU8MWpn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1.2 Questions about Amortized Variational Inference (15 pts)\n",
        "\n",
        "1. **[5 pts]** What do you notice about amortized variational inference (especially as compared with stochastic variational inference)? \n",
        "  * i) Are there any downsides to using the amortized version?\n",
        "  * ii) What do you observe about sample quality and reconstruction quality? \n",
        "  * iii) What do you observe about the ELBO and KL term? \\\\\n",
        "(Here, feel free to vary parameters and compare with the T1.1)\n",
        "2. **[4 pts]** Stochastic and amortized variational inference can be combined, leading to semi-amortized variational inference. Give an instance of an algorithm that would combine these and explain why that would be useful?\n",
        "3. **[1 pts]** What gradient estimation method is used to compute the gradients with respect to the encoder parameters?\n",
        "\n",
        "**[5 pts] Model Implementation and Results**"
      ]
    },
    {
      "metadata": {
        "id": "6UdvLfi1oysb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## T1.3 KL annealing\n",
        "\n",
        "In this section we are going to be looking at the same model as in **T2.1**: same encoder + decoder, prior. But we are going to change slightly the optimization objective as given below.\n",
        "\n",
        "Objective - maximize: \n",
        "\\begin{equation}\n",
        " \\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x) \\log p_\\theta(x|z)}  - \\alpha \\mathbb{E}_{p^*(x)} KL(q(z|x)||p(z))\n",
        "\\end{equation}\n",
        "\n",
        "Where **$\\alpha$ changes during training, to weigh in the KL term more**. In particular for our problem consider:\n",
        "\\begin{equation}\n",
        "\\alpha = \\frac{n_{iter}}{N}\n",
        "\\end{equation}\n",
        "where $n_{iter}$ is the number of training iterations we are have completed and $N$ is the total number of training iterations `TRAINING_STEPS`.\n",
        "\n",
        "**Task**: Implement and train this model to generate MNIST digits. Visualise the results and answer the questions at the end of the section."
      ]
    },
    {
      "metadata": {
        "id": "BF22sRaeBkla",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "metadata": {
        "id": "sqOVwsdFo2PK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zPGuSGA2JHF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_9aPhCEZDp4V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prior = make_prior()\n",
        "decoder = standard_decoder\n",
        "encoder = encoder\n",
        "\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# variational_posterior = ... # Hint: From T2.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cAD9wbYtBpqQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the KL coefficient $\\alpha$ and its update function"
      ]
    },
    {
      "metadata": {
        "id": "sLSjAR9g2ZqV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "#kl_coefficient = ...\n",
        "#kl_step = ...\n",
        "\n",
        "update_kl_coeff = tf.assign(kl_coefficient, kl_coefficient + kl_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQbSvlj92JeG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# Hint: This very similar to what you've done in T1.2. Same model, only slightly different loss including $\\alpha$\n",
        "# loss =  ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4NBpfmRB2Ute",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We now perform joint optimization on the encoder and decoder variables.\n",
        "optimizer = tf.train.AdamOptimizer(0.001, beta1=0.9, beta2=0.9)\n",
        "\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# Parameter update operation (as before)\n",
        "# variables_update_op = ...\n",
        "\n",
        "# Ensure that a variable update is followed by an update in the KL coefficient.\n",
        "with tf.control_dependencies([variables_update_op]):\n",
        "  update_op = tf.identity(update_kl_coeff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0V1KL4wK2Utm"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6-zDc0Lv2Uto",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.initialize_all_variables())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Voo_XAxa2Uts",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %hide_pyerr  # - uncomment to interrupt training without a stacktrace\n",
        "losses = []\n",
        "kls = []\n",
        "likelihood_terms = []\n",
        "\n",
        "\n",
        "for i in xrange(TRAINING_STEPS):\n",
        "  sess.run(update_op)\n",
        "  \n",
        "  if i % 100 == 0:\n",
        "    iteration_loss, iteration_likelihood, iteration_kl  = sess.run(\n",
        "          [loss, likelihood_term, kl_term])\n",
        "    print('Iteration {}. Loss {}. KL {}'.format(\n",
        "          i, iteration_loss, iteration_kl))\n",
        "    losses.append(iteration_loss)\n",
        "    kls.append(iteration_kl)\n",
        "    likelihood_terms.append(iteration_likelihood)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5csVkdYLBHcK"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Let us take a look at the optimization process and the resulting model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BoIEU_9FBHcR"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize training process\n",
        "\n",
        "Plot the loss and KL and likelihood over the training process (number of iterations)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "e0ILfEYa5Z0R",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*8,5))\n",
        "\n",
        "axes[0].plot(losses, label='Negative ELBO')\n",
        "axes[0].set_title('Time', fontsize=15)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(kls, label='KL')\n",
        "axes[1].set_title('Time', fontsize=15)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(likelihood_terms, label='Likelihood Term')\n",
        "axes[2].set_title('Time', fontsize=15)\n",
        "axes[2].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EH1VMkbJBRmc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate samples, reconstructions and latent interpolation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "L4ZYTrg-2UtS",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples = decoder(prior.sample()).mean()\n",
        "samples.shape.assert_is_compatible_with([BATCH_SIZE, 28, 28, 1])\n",
        "\n",
        "reconstructions = decoder(variational_posterior.sample()).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2Knn2ra65Z0a",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data_vals, final_samples_vals, data_reconstructions_vals = sess.run(\n",
        "      [real_data, samples, reconstructions])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "c-v7DO-b5Z0h",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*4,4))\n",
        "\n",
        "\n",
        "show_digits(axes[0], real_data_vals, 'Data')\n",
        "show_digits(axes[1], data_reconstructions_vals, 'Reconstructions')\n",
        "show_digits(axes[2], final_samples_vals, 'Samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nw8H8x2c8zaW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_latent_interpolations(lambda x: decoder(x).mean(), prior, sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ZbZIhli8GBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1.3 Questions about KL annealing (15 pts):\n",
        "\n",
        "1. **[3 pts]** What do you observe about the KL behaviour throughout training as opposed to amortized variational inference without any KL annealing? \n",
        "2.  **[1 pts]** How do the samples and reconstruction compare with the previous models?\n",
        "3.  **[6 pts]** Consider now a schedule where $\\alpha$ increases over time -- that is the contribution of the KL diminishes over time. When would that be a useful case? (Think about what this objective corresponds to in the optimization problem).\n",
        "\n",
        "**[5 pts] Model Implementation and Results**"
      ]
    },
    {
      "metadata": {
        "id": "HXTPxRYeo2k0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## T1.4 Constrained optimization\n",
        "\n",
        "In this next part, instead of using KL annealing, constrained optimization can be used to automatically tuned the relative weight of the likelihood and KL terms. This removes the need to manually create an optimization schedule, which can be problem specific.\n",
        "\n",
        "The objective now becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\text{minimize } \\mathbb{E}_{p^*(x)} KL(q(z|x)||p(z)) \\text{ such that }  \\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} \\left[ {\\log p_\\theta(x|z)} \\right] > \\alpha \n",
        "\\end{equation}\n",
        "\n",
        "This can be solved using the use of Lagrange multipliers. The objective becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\text{minimize }  \\mathbb{E}_{p^*(x)} KL(q(z|x)||p(z)) + \\lambda  (\\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} (\\alpha - \\log p_\\theta(x|z)))\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The difference compared to the KL annealing is that:\n",
        "\n",
        "   * $\\lambda$ is a learned parameter - it will be learned using stochastic gradient descent, like the network parameters.  The difference is that the lagrangian has to solve a maximization problem. You can see this intuitively: the gradient with respect to $\\lambda$ in the objective above is $\\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} (\\alpha - \\log p_\\theta(x|z))$. If $ \\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} (\\alpha - \\log p_\\theta(x|z))> 0$, the constraint is not being satisfied, so the value of the lagrangian needs to increase. This will be done by doing gradient ascent, instead of gradient descent. Note that for $\\lambda$ to be a valid lagranian in a minimization problem, it has to be positive.\n",
        "   * The practicioner has to specify the hyperparameter $\\alpha$, which determines the reoncstruction quality of the model.\n",
        "   * the coefficient is in front of the likelihood term, not the KL term. This is mainly for convenience, as it is easier to specify the hyperparameter $\\alpha$ for the likelihood (reconstruction loss).\n",
        "\n",
        "\n",
        "For more assumptions made by this method, see the Karush–Kuhn–Tucker conditions.\n",
        "\n",
        "For more information, see: \n",
        "  * http://bayesiandeeplearning.org/2018/papers/33.pdf\n",
        "\n",
        "**Task**: Implement and train this model to generate MNIST digits. Visualise the results and answer the questions at the end of the section."
      ]
    },
    {
      "metadata": {
        "id": "nIYXEQzeEhHk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Q-Ew9-U_EqvB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "sIBgjVgPEqvH",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ITvJG0hdEqvL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prior = make_prior()\n",
        "decoder = standard_decoder\n",
        "encoder = encoder\n",
        "\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# variational_posterior = ... # Hint: From T2.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9tFkuj9FByi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the lagrangian variable $\\lambda$.\n",
        "Unlike in the KL annealing case, we learn the coefficient. Remember that this variable has to be always positive. To ensure this, use `tf.nn.softplus`, Moreover, please initialize the lagrangian such that after the softplus the coefficient is approximately 1. Check emperically that this is true when instantiating the variable."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-kJd3IKCKkt5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "#lagrangian_var = ...\n",
        "\n",
        "# Ensure that the lagrangian is positive and has stable dynamics.\n",
        "lagrangian = tf.nn.softplus(lagrangian_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7y7SkHsALzZn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# How good do we want the reconstruction loss to be?\n",
        "# We can look at previous runs to get an idea what a reasonable value would be.\n",
        "\n",
        "##################\n",
        "# YOUR VALUE HERE#\n",
        "##################\n",
        "# reconstruction_target = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRwUZb0_GJDO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the loss"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XG45n9zkKkt9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# \n",
        "# loss =  kl_term + lagrangian * (reconstruction_target - likelihood_term )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q1e2b1BrOcW4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check trainable variables (the lagrangian variable should be in here)\n",
        "tf.trainable_variables()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ItrKEIEQKkuG",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lagrangian_optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
        "\n",
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# autoencoder_variables_update_op = ...\n",
        "\n",
        "# Ensure that a variable update is followed by an update to the Lagrangian.\n",
        "with tf.control_dependencies([autoencoder_variables_update_op]):\n",
        "  # Ensure that the lagrangian solves a maximization problem instead of a\n",
        "  # minimization problem by changing the sign of the loss function.\n",
        "  update_op = lagrangian_optimizer.minimize(- loss, var_list=[lagrangian_var])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BhWHvrlBKkuJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Yp1z9724KkuK",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.initialize_all_variables())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "phV3D_PNKkuO",
        "outputId": "b9872fd0-4988-44e3-a315-c0674bf9dc6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1817
        }
      },
      "cell_type": "code",
      "source": [
        "# %hide_pyerr  # - uncomment to interrupt training without a stacktrace\n",
        "losses = []\n",
        "kls = []\n",
        "likelihood_terms = []\n",
        "lagrangian_values = []\n",
        "\n",
        "for i in xrange(TRAINING_STEPS):\n",
        "  sess.run(update_op)\n",
        "  \n",
        "  \n",
        "  if i % 100 == 0:\n",
        "    iteration_loss, iteration_likelihood, iteration_kl, lag_val  = sess.run(\n",
        "          [loss, likelihood_term, kl_term, lagrangian])\n",
        "    print('Iteration {}. Loss {}. KL {}. Lagrangian {}'.format(\n",
        "          i, iteration_loss, iteration_kl, lag_val))\n",
        "    losses.append(iteration_loss)\n",
        "    kls.append(iteration_kl)\n",
        "    likelihood_terms.append(iteration_likelihood)\n",
        "    lagrangian_values.append(lag_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0. Loss [541.7995]. KL 0.0352992638946. Lagrangian [1.197807]\n",
            "Iteration 100. Loss [2079.5444]. KL 23.6862182617. Lagrangian [19.252846]\n",
            "Iteration 200. Loss [1809.995]. KL 44.9460372925. Lagrangian [28.361876]\n",
            "Iteration 300. Loss [982.47107]. KL 54.9215202332. Lagrangian [32.87678]\n",
            "Iteration 400. Loss [680.7418]. KL 60.2341461182. Lagrangian [34.964283]\n",
            "Iteration 500. Loss [596.9203]. KL 56.2811889648. Lagrangian [36.20462]\n",
            "Iteration 600. Loss [115.77269]. KL 58.1710243225. Lagrangian [36.948597]\n",
            "Iteration 700. Loss [210.97853]. KL 57.4608154297. Lagrangian [37.513374]\n",
            "Iteration 800. Loss [136.11151]. KL 51.4106674194. Lagrangian [37.803387]\n",
            "Iteration 900. Loss [58.05602]. KL 53.627204895. Lagrangian [37.95315]\n",
            "Iteration 1000. Loss [-51.688946]. KL 51.8700370789. Lagrangian [37.951996]\n",
            "Iteration 1100. Loss [151.58333]. KL 50.7084274292. Lagrangian [37.86333]\n",
            "Iteration 1200. Loss [-135.017]. KL 46.5962524414. Lagrangian [37.711273]\n",
            "Iteration 1300. Loss [32.000687]. KL 49.9308853149. Lagrangian [37.478817]\n",
            "Iteration 1400. Loss [-116.43298]. KL 49.2396583557. Lagrangian [37.110218]\n",
            "Iteration 1500. Loss [-168.37144]. KL 49.0758171082. Lagrangian [36.737503]\n",
            "Iteration 1600. Loss [-19.992836]. KL 45.9241333008. Lagrangian [36.28124]\n",
            "Iteration 1700. Loss [-261.8975]. KL 43.7975120544. Lagrangian [35.83392]\n",
            "Iteration 1800. Loss [-101.60282]. KL 47.7132949829. Lagrangian [35.234978]\n",
            "Iteration 1900. Loss [-204.36224]. KL 45.5789337158. Lagrangian [34.702778]\n",
            "Iteration 2000. Loss [-42.759132]. KL 46.159034729. Lagrangian [34.081306]\n",
            "Iteration 2100. Loss [-284.24768]. KL 46.3801574707. Lagrangian [33.371338]\n",
            "Iteration 2200. Loss [-183.06717]. KL 44.3533935547. Lagrangian [32.71816]\n",
            "Iteration 2300. Loss [-29.749825]. KL 45.9133453369. Lagrangian [31.986]\n",
            "Iteration 2400. Loss [-319.1849]. KL 43.3486213684. Lagrangian [31.233154]\n",
            "Iteration 2500. Loss [-159.78786]. KL 42.0662231445. Lagrangian [30.504232]\n",
            "Iteration 2600. Loss [-202.27539]. KL 42.9071578979. Lagrangian [29.715223]\n",
            "Iteration 2700. Loss [-214.05893]. KL 41.4365158081. Lagrangian [28.887203]\n",
            "Iteration 2800. Loss [-269.72375]. KL 42.4422607422. Lagrangian [28.050806]\n",
            "Iteration 2900. Loss [-283.41443]. KL 41.647026062. Lagrangian [27.18032]\n",
            "Iteration 3000. Loss [-133.74292]. KL 40.6553421021. Lagrangian [26.352976]\n",
            "Iteration 3100. Loss [-152.64445]. KL 41.2285957336. Lagrangian [25.46095]\n",
            "Iteration 3200. Loss [-96.81888]. KL 41.5986289978. Lagrangian [24.491226]\n",
            "Iteration 3300. Loss [-196.9295]. KL 42.6475219727. Lagrangian [23.551863]\n",
            "Iteration 3400. Loss [-153.52452]. KL 40.165599823. Lagrangian [22.673553]\n",
            "Iteration 3500. Loss [-151.06546]. KL 37.5955734253. Lagrangian [21.687883]\n",
            "Iteration 3600. Loss [-229.68317]. KL 39.9825668335. Lagrangian [20.716698]\n",
            "Iteration 3700. Loss [-104.87711]. KL 38.4817276001. Lagrangian [19.743465]\n",
            "Iteration 3800. Loss [-105.99178]. KL 37.782875061. Lagrangian [18.733461]\n",
            "Iteration 3900. Loss [-249.00357]. KL 38.3341903687. Lagrangian [17.74827]\n",
            "Iteration 4000. Loss [-68.69702]. KL 36.1678352356. Lagrangian [16.70795]\n",
            "Iteration 4100. Loss [-174.97833]. KL 39.7596549988. Lagrangian [15.695165]\n",
            "Iteration 4200. Loss [-155.16246]. KL 34.0775718689. Lagrangian [14.718307]\n",
            "Iteration 4300. Loss [-66.7576]. KL 37.329624176. Lagrangian [13.623195]\n",
            "Iteration 4400. Loss [-108.89918]. KL 35.0873794556. Lagrangian [12.51753]\n",
            "Iteration 4500. Loss [-118.21915]. KL 33.5963401794. Lagrangian [11.489143]\n",
            "Iteration 4600. Loss [-53.56543]. KL 34.0425567627. Lagrangian [10.457892]\n",
            "Iteration 4700. Loss [-26.112885]. KL 34.0569229126. Lagrangian [9.426848]\n",
            "Iteration 4800. Loss [-18.935707]. KL 32.3330039978. Lagrangian [8.370506]\n",
            "Iteration 4900. Loss [-36.000717]. KL 31.3581390381. Lagrangian [7.2873397]\n",
            "Iteration 5000. Loss [-12.562384]. KL 31.8118114471. Lagrangian [6.20336]\n",
            "Iteration 5100. Loss [-7.5185757]. KL 29.1720752716. Lagrangian [5.173302]\n",
            "Iteration 5200. Loss [-15.894596]. KL 30.4736557007. Lagrangian [4.1561613]\n",
            "Iteration 5300. Loss [8.029518]. KL 26.8174228668. Lagrangian [3.1686027]\n",
            "Iteration 5400. Loss [25.604906]. KL 26.1411895752. Lagrangian [2.3228507]\n",
            "Iteration 5500. Loss [6.5341053]. KL 22.5977516174. Lagrangian [1.658878]\n",
            "Iteration 5600. Loss [13.560015]. KL 22.0133094788. Lagrangian [1.227013]\n",
            "Iteration 5700. Loss [9.776123]. KL 19.2209739685. Lagrangian [0.94283783]\n",
            "Iteration 5800. Loss [17.78655]. KL 19.883392334. Lagrangian [0.7869422]\n",
            "Iteration 5900. Loss [16.17637]. KL 18.150560379. Lagrangian [0.6876382]\n",
            "Iteration 6000. Loss [17.821135]. KL 17.0874729156. Lagrangian [0.6272664]\n",
            "Iteration 6100. Loss [15.111039]. KL 16.5076503754. Lagrangian [0.574869]\n",
            "Iteration 6200. Loss [16.96215]. KL 16.7488555908. Lagrangian [0.5575547]\n",
            "Iteration 6300. Loss [16.180567]. KL 16.7198677063. Lagrangian [0.54117465]\n",
            "Iteration 6400. Loss [14.43391]. KL 15.754486084. Lagrangian [0.5468288]\n",
            "Iteration 6500. Loss [19.180103]. KL 16.8049125671. Lagrangian [0.5451356]\n",
            "Iteration 6600. Loss [12.784924]. KL 15.6518287659. Lagrangian [0.5357091]\n",
            "Iteration 6700. Loss [16.608559]. KL 16.9377422333. Lagrangian [0.5367914]\n",
            "Iteration 6800. Loss [16.740673]. KL 15.5889787674. Lagrangian [0.53593206]\n",
            "Iteration 6900. Loss [10.609623]. KL 15.0649805069. Lagrangian [0.5185327]\n",
            "Iteration 7000. Loss [16.558035]. KL 16.1625747681. Lagrangian [0.51629287]\n",
            "Iteration 7100. Loss [17.005291]. KL 15.8910598755. Lagrangian [0.51144433]\n",
            "Iteration 7200. Loss [16.193079]. KL 15.6455135345. Lagrangian [0.516937]\n",
            "Iteration 7300. Loss [17.056091]. KL 15.51628685. Lagrangian [0.51827925]\n",
            "Iteration 7400. Loss [14.588214]. KL 15.8725547791. Lagrangian [0.5180299]\n",
            "Iteration 7500. Loss [14.940847]. KL 16.0428085327. Lagrangian [0.5280319]\n",
            "Iteration 7600. Loss [13.2639065]. KL 14.7286262512. Lagrangian [0.5275872]\n",
            "Iteration 7700. Loss [15.796695]. KL 16.0446853638. Lagrangian [0.5240154]\n",
            "Iteration 7800. Loss [16.879593]. KL 15.6523971558. Lagrangian [0.5115347]\n",
            "Iteration 7900. Loss [15.15191]. KL 15.8156938553. Lagrangian [0.5143325]\n",
            "Iteration 8000. Loss [16.134165]. KL 16.0066623688. Lagrangian [0.513113]\n",
            "Iteration 8100. Loss [13.667096]. KL 15.0862979889. Lagrangian [0.51661235]\n",
            "Iteration 8200. Loss [13.566738]. KL 16.448217392. Lagrangian [0.5129573]\n",
            "Iteration 8300. Loss [17.88404]. KL 15.3343410492. Lagrangian [0.5109427]\n",
            "Iteration 8400. Loss [10.400753]. KL 15.0261154175. Lagrangian [0.5113629]\n",
            "Iteration 8500. Loss [15.084027]. KL 15.5496006012. Lagrangian [0.5062438]\n",
            "Iteration 8600. Loss [15.508322]. KL 15.0626335144. Lagrangian [0.4987896]\n",
            "Iteration 8700. Loss [13.834112]. KL 14.5499210358. Lagrangian [0.49169123]\n",
            "Iteration 8800. Loss [16.373087]. KL 15.8138771057. Lagrangian [0.4988923]\n",
            "Iteration 8900. Loss [16.049427]. KL 15.4969863892. Lagrangian [0.4991973]\n",
            "Iteration 9000. Loss [14.9651165]. KL 15.0618896484. Lagrangian [0.49420357]\n",
            "Iteration 9100. Loss [14.640309]. KL 15.2749557495. Lagrangian [0.49794906]\n",
            "Iteration 9200. Loss [16.911331]. KL 14.5537433624. Lagrangian [0.49588507]\n",
            "Iteration 9300. Loss [15.264124]. KL 15.9398488998. Lagrangian [0.49344075]\n",
            "Iteration 9400. Loss [18.282375]. KL 15.1034412384. Lagrangian [0.49902657]\n",
            "Iteration 9500. Loss [11.54266]. KL 14.602973938. Lagrangian [0.49189723]\n",
            "Iteration 9600. Loss [18.094093]. KL 15.3920249939. Lagrangian [0.48762178]\n",
            "Iteration 9700. Loss [13.533478]. KL 14.4577045441. Lagrangian [0.49118614]\n",
            "Iteration 9800. Loss [12.521185]. KL 14.5473852158. Lagrangian [0.4888521]\n",
            "Iteration 9900. Loss [15.116826]. KL 15.0607624054. Lagrangian [0.49271715]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GAlq4-MQNE9l"
      },
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "\n",
        "Let us take a look at the optimization process and the resulting model"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "_T1p-btINE9o"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize training process\n",
        "\n",
        "Plot the loss and KL over the training process (number of iterations)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Aj6HI5uqKkuW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(2*8, 2* 5))\n",
        "\n",
        "axes[0, 0].plot(losses, label='Negative ELBO')\n",
        "axes[0, 0].set_title('Time', fontsize=15)\n",
        "axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].plot(kls, label='KL')\n",
        "axes[0, 1].set_title('Time', fontsize=15)\n",
        "axes[0, 1].legend()\n",
        "\n",
        "axes[1, 0].plot(likelihood_terms, label='Likelihood Term')\n",
        "axes[1, 0].set_title('Time', fontsize=15)\n",
        "axes[1, 0].legend()\n",
        "\n",
        "axes[1, 1].plot(lagrangian_values, label='Lagrangian Values')\n",
        "axes[1, 1].set_title('Time', fontsize=15)\n",
        "axes[1, 1].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uUoBwfHdKkuB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples = decoder(prior.sample()).mean()\n",
        "samples.shape.assert_is_compatible_with([BATCH_SIZE, 28, 28, 1])\n",
        "\n",
        "reconstructions = decoder(variational_posterior.sample()).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ys5QOAYH8uvK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate samples and latent interpolations"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0EP9dSqfKkua",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data_vals, final_samples_vals, data_reconstructions_vals = sess.run(\n",
        "      [real_data, samples, reconstructions])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UpqpNO87Kkud",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*4,4))\n",
        "\n",
        "\n",
        "show_digits(axes[0], real_data_vals, 'Data')\n",
        "show_digits(axes[1], data_reconstructions_vals, 'Reconstructions')\n",
        "show_digits(axes[2], final_samples_vals, 'Samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uw_VtACw8uD4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_latent_interpolations(lambda x: decoder(x).mean(), prior, sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HREJ6CfDWRBA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q4.1 Questions about constrained optimization (12 pts)\n",
        "\n",
        "1.  **[3 pts]** Based on previous results, set and try varying the threshold for reconstruction (variable `reconstruction_target` int the code above). Describe what happens when you vary this variable. (Produce the plots to support your answer)\n",
        "2.  **[1 pts]** What do you observe about the behaviour of the likelihood and KL term throughout training? How is it different than in Stochastic Variational Inference and Amortized Variational inference with and without KL annealing?\n",
        "3.  **[3 pts]** What do you notice about the behaviour of the lagrangian during training? Is that what you expected?\n",
        "\n",
        "\n",
        "**[5 pts] Model Implementation and Results**"
      ]
    },
    {
      "metadata": {
        "id": "ozkyBGCXNnty",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2: Reconstruction-free Generative Models"
      ]
    },
    {
      "metadata": {
        "id": "IibusvDaNIUE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  T2.1 Generative Adversarial Networks\n",
        "\n",
        "So far we have discussed variational inference models, which learn how to match the marginal distribution $p_\\theta(x)$, learned by the model, with the true data distribution $p^\\star(x)$ through the variational lower bound. This approach uses latent variables and requires that the conditional posterior distributions cover the prior space; otherwise the decoder will not be able to generalise to prior samples which are unlike what it has seen during training. \n",
        "\n",
        "\n",
        "To avoid this issue, some methods directly match $p_\\theta(x)$ learned by the model with the true data distribution $p^\\star(x)$. Such an approach is given by generative adversarial networks (GANs).\n",
        "\n",
        "Generative adversarial networks optimize an adversarial two-player game given by the value function:\n",
        "\\begin{equation}\n",
        "\\max_{G} \\min_{D} \\mathbb{E}_{p^*(x)} \\log D(x) + \\mathbb{E}_{p(z)} \\log (1- D(G(z)))\n",
        "\\end{equation}\n",
        "where $G$ is the generator (as before this takes a latent sample $z$ and produce a image $x_{gen}$) and $D$ denotes the discriminator. \n",
        "\n",
        "A depiction of the model can be found below:\n",
        "\n",
        "<h2 align=\"center\"></h2> <img src=\"http://elarosca.net/gan.png?format=100w\" width=210 >\n",
        "\n",
        "For more information, see: \n",
        "  * https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\n",
        "  * https://arxiv.org/abs/1701.00160\n",
        "\n",
        "**Task**: Implement and train this model to generate MNIST digits. Visualise the results and answer the questions at the end of the section.\n"
      ]
    },
    {
      "metadata": {
        "id": "8MtMnfBUyrPa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q2.1 Prelimary questions (5 pts)\n",
        "\n",
        "Before trying to implement this generative model, let us take a closer look at its components and how one can train them. \n",
        "*  **[2 pts]** First thing to note is that now we have two models to train: a generative model $G$ and a discriminative model $D$. How does one train these two models in the (standard) GAN formulation? Give the update rules and losses for each these ($D$ and $G$). Which of these losses uses the generated data and which of them uses the real data.\n",
        "*  **[3 pts]** [Generator loss] Instead of using the generator loss above, in practice we often use a surrogate,  $- \\log D(G(x))$. Why do you think that is the case? Plot the original loss and associated gradients. Then plot the surgate loss $- \\log D(G(x))$ and its associated gradients."
      ]
    },
    {
      "metadata": {
        "id": "HrgX4MZ71YYK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "metadata": {
        "id": "vpgRrCpnnnl8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "riBWlK6Cnx-9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get and rescale the data\n",
        "\n",
        "Scale the data between -1 and 1. This helps training stability and improves GAN convergence."
      ]
    },
    {
      "metadata": {
        "id": "RRQXcxJZnoLP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)\n",
        "real_data = 2 * real_data - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OMlVJgu_mQ-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the discriminator and generator networks\n",
        "\n",
        "We will use the same network as the VAE decoder for the generator. The only difference is that the generator here is implicit - it does not define a probability distribution over pixels. Since the input data is scaled to be between -1, and 1, the generator output range will be the same, by using a tanh output non-linearty."
      ]
    },
    {
      "metadata": {
        "id": "AM35_0c2lFNK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DISCRIMINATOR_VARIABLE_SCOPE = 'discriminator'\n",
        "GENERATOR_VARIABLE_SCOPE = 'generator'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NH06lhJIk7ZW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminator(x):\n",
        "  with tf.variable_scope(DISCRIMINATOR_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
        "    h = x\n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=8,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(x)\n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=16,\n",
        "          kernel_size=5,\n",
        "          strides=1,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=32,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=64,\n",
        "          kernel_size=5,\n",
        "          strides=1,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=64,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h) \n",
        "    \n",
        "    out_shape = 1\n",
        "    for s in h.shape.as_list()[1:]:\n",
        "      out_shape*= s\n",
        "    \n",
        "    h = tf.reshape(h, shape=[BATCH_SIZE, out_shape])\n",
        "    logits = tf.layers.dense(h, 1, activation=None)\n",
        "    return logits\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-erco21ksOl8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(z):\n",
        "  with tf.variable_scope(GENERATOR_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
        "    h = tf.layers.dense(z, 7 * 7 * 64, activation=tf.nn.relu)\n",
        "    h = tf.reshape(h, shape=[BATCH_SIZE, 7, 7, 64])\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=32,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=1,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=None,  # Do not activate the last layer.\n",
        "          padding='same')(h)\n",
        "    return tf.nn.tanh(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hEBUp7pNopKc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate samples"
      ]
    },
    {
      "metadata": {
        "id": "8nBMaTWfovpn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# samples = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgrZ6gL2o4B-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up the adversarial game"
      ]
    },
    {
      "metadata": {
        "id": "bfr9_pW6pC7h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Discriminator loss"
      ]
    },
    {
      "metadata": {
        "id": "X5TUfafDo3oQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "\n",
        "# Reduce loss over batch dimension\n",
        "# discriminator_loss = tf.reduce_mean(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yp1OvtmR2Hls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generator loss"
      ]
    },
    {
      "metadata": {
        "id": "48_nvS8qJGIZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# generator_loss = tf.reduce_mean(...)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WU0MVP0BJKK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create optimizers and training ops\n",
        "\n",
        "**Important**: You will need to pass the list of variables to the TensorFlow optimizer, otherwise the generator and discriminator\n",
        "variables will receive both the loss of the discrimiantor and that of the generator. \n",
        "\n",
        "We want to freeze the discriminator when we update the generator, and vice versa."
      ]
    },
    {
      "metadata": {
        "id": "0pgqta3JLRBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "discriminator_optimizer = tf.train.AdamOptimizer(0.0001, beta1=0.5, beta2=0.9)\n",
        "generator_optimizer = tf.train.AdamOptimizer(0.0003, beta1=0.9, beta2=0.9)\n",
        "\n",
        "# Optimize the discrimiantor.\n",
        "discriminator_vars =  tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=DISCRIMINATOR_VARIABLE_SCOPE)\n",
        "discriminator_update_op = discriminator_optimizer.minimize(\n",
        "    discriminator_loss, var_list=discriminator_vars)\n",
        "\n",
        "# Optimize the generator.\n",
        "generator_vars =  tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=GENERATOR_VARIABLE_SCOPE)\n",
        "generator_update_op = generator_optimizer.minimize(\n",
        "    generator_loss, var_list=generator_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "el1bUubX2qhl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n"
      ]
    },
    {
      "metadata": {
        "id": "73AZS7Rfbxj1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create the tensorflow session"
      ]
    },
    {
      "metadata": {
        "id": "3Qvja62rEClx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrW5Nrh7b3w1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Loop\n",
        "\n",
        "We train the discriminator and generator by alternating gradient descent runs. We record the losses to plot them later."
      ]
    },
    {
      "metadata": {
        "id": "-LTMPqr-dL3z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %hide_pyerr  # - uncomment to interrupt training without a stacktrace\n",
        "disc_losses = []\n",
        "gen_losses = []\n",
        "\n",
        "for i in xrange(TRAINING_STEPS):\n",
        "  sess.run(discriminator_update_op)\n",
        "  sess.run(generator_update_op)\n",
        "  \n",
        "  if i % 100 == 0: \n",
        "    disc_loss = sess.run(discriminator_loss)\n",
        "    gen_loss = sess.run(generator_loss)\n",
        "    \n",
        "    print('Iteration: {}. Disc loss: {}. Generator loss {}'.format(\n",
        "          i, disc_loss, gen_loss))\n",
        "  \n",
        "    disc_losses.append(disc_loss)\n",
        "    gen_losses.append(gen_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zV3spDTANZDK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results"
      ]
    },
    {
      "metadata": {
        "id": "VA5MrVNJhXKR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the behaviour of the two losses during training\n",
        "\n",
        "Note that unlike losses for classifiers, or for VAEs, the losses are not stable and are going up and down, depending on the training dynamics. "
      ]
    },
    {
      "metadata": {
        "id": "87iLGbDbh6bV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "figsize = (18, 4)\n",
        "fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "\n",
        "# First plot the loss, and then the derivative.\n",
        "axs[0].plot(disc_losses, '-')\n",
        "axs[0].plot([np.log(2)] * len(disc_losses), 'r--', label='Discriminator is being folled')\n",
        "axs[0].legend()\n",
        "axs[0].set_title('Discriminator loss', fontsize=20, y=-0.2)\n",
        "axs[1].plot(gen_losses, '-')\n",
        "axs[1].set_title('Generator loss', fontsize=20, y=-0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bN5kwtHI1LRW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate and plot samples and latent interpolations"
      ]
    },
    {
      "metadata": {
        "id": "Hx5YOpW51IJb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data_vals, final_samples_vals  = sess.run([real_data, samples])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n64ma5xfpf1B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(2*4,4))\n",
        "\n",
        "show_digits(axes[0], real_data_vals, 'Data')\n",
        "show_digits(axes[1], final_samples_vals, 'Samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NXHKM0vf8jXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_latent_interpolations(generator, prior, sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBrXn2bMKKAy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q2.2 GAN Questions (25 pts):  \n",
        "1.  **[3 pts]** In defining the optimization procedure above (Section **Create optimizers and training ops**) we opted for two optimizer one for the discriminator and one for the generator. Is this necessary? Why would this be a good/bad idea in general? \n",
        "\n",
        "2.  **[3 pts]** Discuss the hyperparameter sensitivy of GANs compared to that of VAEs. (What happens to the model if you use a higher learning rate for the discriminator or the generator?)\n",
        "\n",
        "3.  **[2 pts]** When would you want to use GANs and when would you want to use VAEs? Which of the following can be performed using VAEs, and which can be performed using GANs: density estimation, representation learning, data generation?\n",
        "4.  **[2 pts]** What do you observe about GAN samples compared to VAE samples?\n",
        "5.  **[3 pts]** What happens if you optimize the GAN discriminator 5 times per generator update? (This will become particularly relevant for next part) \n",
        "6.  **[2 pts]** What happens if you optimize the GAN generator 10 times per discriminator update?\n",
        "\n",
        "**[10 pts] Model Implementation and Results**\n"
      ]
    },
    {
      "metadata": {
        "id": "2H7iJXclKsZV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ===== END OF GRADED COURSEWORK ========"
      ]
    },
    {
      "metadata": {
        "id": "5ylp0c9jyX7r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## T2.2 [Optional] Wasserstein GAN\n",
        "\n",
        "Since the proposal of the original GAN, multiple objectives have been proposed, inspired by different learning principles. In Wasserstein GAN, optimal transport is used to create the training criteria:\n",
        "\\begin{equation}\n",
        "  \\sup_{||f||_{L \\leq 1}} \\mathbb{E}_{p^*(x)} f(x) - \\mathbb{E}_{p_(z)} f(G(z))\n",
        "\\end{equation}\n",
        "where $||f||_{L \\leq 1}$ denotes the family of 1-Lipchitz functions.\n",
        "\n",
        "Due to the intractability of the supremum in the equation above, the WGAN value function is constructed from the optimal transport criteria using the Kantorovich-Rubinstein duality:\n",
        "\\begin{equation}\n",
        "  \\min_{G} \\max_{D}  \\mathbb{E}_{p^*(x)} D(x) - \\mathbb{E}_{p_(z)} D(G(z))\n",
        "\\end{equation}\n",
        "\n",
        "where $D$ is a 1-Lipchitz function. The Lipchitz constraint is imposed using gradient penalties on the discriminator. \n",
        "\n",
        "For extra reading, see:\n",
        " * https://arxiv.org/abs/1701.07875\n",
        " * https://arxiv.org/abs/1704.00028\n",
        "\n",
        "\n",
        "**Task**: Implement and train this model to generate MNIST digits. Visualise the results and answer the questions at the end of the section."
      ]
    },
    {
      "metadata": {
        "id": "S1y5KidNKNjo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "KDArsamPytnc",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y0UWEuQi0WJW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define the gradient penalty"
      ]
    },
    {
      "metadata": {
        "id": "U1k648KD0Y1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_l2_norms(x, eps=1e-5):\n",
        "  reduction_axis = range(1, x.get_shape().ndims)\n",
        "  squares = tf.reduce_sum(tf.square(x), axis=reduction_axis)\n",
        "  squares.get_shape().assert_is_compatible_with([None])\n",
        "  return tf.sqrt(eps + squares)\n",
        "\n",
        "def wgan_gradient_penalty(discriminator, real_data, samples):\n",
        "  \"\"\"The gradient penalty loss on an interpolation of data and samples.\n",
        "\n",
        "  Proposed by https://arxiv.org/pdf/1704.00028.pdf for Wasserstein GAN, but\n",
        "  recently becoming more widely adopted, outside the Wasserstein setting.\n",
        "\n",
        "  Args:\n",
        "    discriminator: An instance of `AbstractDiscriminator`.\n",
        "    real_data: A `tf.Tensor` (joint discriminator `tf.Tensor` sequences are not\n",
        "      yet supported). The data associated as real by the GAN, usually from a\n",
        "      datasets. Needs to be a valid input for `discriminator`.\n",
        "    samples: A `tf.Tensor` or `tf.Tensor` sequence (for joint discriminators).\n",
        "      Samples obtained from the model. Needs to be a valid input for\n",
        "      `discriminator`.\n",
        "\n",
        "  Returns:\n",
        "    A `tf.Tensor` scalar, containing the loss.\n",
        "  \"\"\"\n",
        "\n",
        "  ##################\n",
        "  # YOUR CODE HERE #\n",
        "  ##################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "MkpSg0MTytnh"
      },
      "cell_type": "markdown",
      "source": [
        "### Get and rescale the data\n",
        "\n",
        "Scale the data between -1 and 1. This helps training stability and improves GAN convergence."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Az6FHG56ytni",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)\n",
        "real_data = 2 * real_data - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "m7wxX8OZzDkI"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate samples"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6gc_CRPBzDkN",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# samples = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "L_DF7_BZzDkW"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up the adversarial game"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "q84OwrMDzDka"
      },
      "cell_type": "markdown",
      "source": [
        "### Discriminator and generator loss"
      ]
    },
    {
      "metadata": {
        "id": "dOKa9tQF024D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The weight of the gradient penalty\n",
        "GRADIENT_PENALTY_COEFF = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OFkOcyjfzDkd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################\n",
        "# YOUR CODE HERE #\n",
        "##################\n",
        "# discriminator_loss = \n",
        "# generator_loss = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1z1iFzHFzhCv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create optimizers and training ops\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9Z-Pw2g3zYPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "discrimiantor_optimizer = tf.train.AdamOptimizer(0.0001, beta1=0.5, beta2=0.9)\n",
        "generator_optimizer = tf.train.AdamOptimizer(0.0001, beta1=0.5, beta2=0.9)\n",
        "\n",
        "# Optimize the discrimiantor.\n",
        "discriminator_vars =  tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=DISCRIMINATOR_VARIABLE_SCOPE)\n",
        "discriminator_update_op = discrimiantor_optimizer.minimize(\n",
        "    discriminator_loss, var_list=discriminator_vars)\n",
        "\n",
        "# Optimize the generator.\n",
        "generator_vars =  tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=GENERATOR_VARIABLE_SCOPE)\n",
        "generator_update_op = generator_optimizer.minimize(\n",
        "    generator_loss, var_list=generator_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qFobF5vLznpb"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "We train the discriminator and generator by alternating gradient descent runs. We record the losses to plot them later."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "IhwKXrWPznpS"
      },
      "cell_type": "markdown",
      "source": [
        "### Create the tensorflow session"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "YIEmibgcznpW",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cGIBjPvS0Cwp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_DISC_UPDATES_PER_GEN_UPDATE = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7S9mze5jznpd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# %hide_pyerr  # - uncomment to interrupt training without a stacktrace\n",
        "disc_losses = []\n",
        "gen_losses = []\n",
        "\n",
        "for i in xrange(TRAINING_STEPS):\n",
        "  # Do multiple discriminator updates per generator update. \n",
        "  for _ in xrange(NUM_DISC_UPDATES_PER_GEN_UPDATE):\n",
        "    sess.run(discriminator_update_op)\n",
        "  sess.run(generator_update_op)\n",
        "  \n",
        "  if i % 100 == 0: \n",
        "    disc_loss = sess.run(discriminator_loss)\n",
        "    gen_loss = sess.run(generator_loss)    \n",
        "  \n",
        "    print('Iteration: {}. Disc loss: {}. Generator loss {}'.format(\n",
        "          i, disc_loss, gen_loss))\n",
        "    disc_losses.append(disc_loss)\n",
        "    gen_losses.append(gen_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEZwzU-hND3B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Results"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "B9vFkwbpznpi"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize the behaviour of the two losses during training\n",
        "\n",
        "Note that unlike losses for classifiers, or for VAEs, the losses are not stable and are going up and down, depending on the training dynamics. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "q2Y4-1cPznpk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "figsize = (20, 6)\n",
        "fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "\n",
        "# First plot the loss, and then the derivative.\n",
        "axs[0].plot(disc_losses, '-')\n",
        "axs[0].plot([0.] * len(disc_losses), 'r--', label='Discriminator is being folled')\n",
        "axs[0].legend()\n",
        "axs[0].set_title('Discriminator loss', fontsize=20, y=-0.2)\n",
        "axs[1].plot(gen_losses, '-')\n",
        "axs[1].set_title('Generator loss', fontsize=20, y=-0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wMwlF3bC8WFR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate samples and latent interpolations"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MNHbGy2O1aKp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "real_data_vals, final_samples_vals  = sess.run([real_data, samples])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "JoX9r9SU1aKt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(2*4,4))\n",
        "\n",
        "\n",
        "show_digits(axes[0], real_data_vals, 'Data')\n",
        "show_digits(axes[1], final_samples_vals, 'Samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_76kJ-t6rIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "show_latent_interpolations(generator, prior, sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CH1Cbd1LZkbo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Questions about WGANs:\n",
        "\n",
        "* What happens if you optimize the original GAN discriminator 5 times per generator update - like in Wasserstein GANs? What happens if you train the Wassterstein GAN with 1 discriminator update per generator update?\n",
        "* Can you think of a general recipe to create a new type of GAN loss?\n"
      ]
    },
    {
      "metadata": {
        "id": "7O7M4d1VosBv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}